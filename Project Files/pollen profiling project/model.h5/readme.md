A quantized TensorFlow Lite (.tflite) model has been shared in place of the original .h5 model to reduce file size and ensure efficient deployment. The model was converted using post-training quantization techniques to optimize it for inference on resource-constrained environments. While the original training and evaluation were performed using a full-precision Keras model, the final .tflite format was chosen for its portability and compatibility with web and mobile applications.
We sincerely apologize for not including the original .h5 model due to its large size. However, all training steps, model architecture, and conversion details have been thoroughly documented in the accompanying notebook. In addition, the Flask application and prediction logic have been updated to work seamlessly with the provided .tflite model.
